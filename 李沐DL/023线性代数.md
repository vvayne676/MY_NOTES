## 2.3 线性代数
### 2.3.1 标量 Scalars
标量由只有一个元素的张量表示。 下面的代码将实例化两个标量，并执行一些熟悉的算术运算，即加法、乘法、除法和指数。
```python
import torch

x = torch.tensor(3.0)
y = torch.tensor(2.0)

x + y, x * y, x / y, x**y


# (tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))
```

### 2.3.2 Vectors
For current purposes, you can think of a vector as a fixed-length array of scalars.
```python
x = torch.arange(3)
x
# tensor([0, 1, 2])

x[2]
# tensor(2)
len(x)
# 3
x.shape
# torch.Size([3])
```
Oftentimes, the word "dimension" gets overloaded to mean both the number of axes and the length along a particular axis. To avoid this confusion, we use order to refer to the number of axes and dimensionality exclusively to refer to the number of components.
### 2.3.3 矩阵
当调用函数来实例化张量时， 我们可以通过指定两个分量 m 和 n 来创建一个形状为 m * n 的矩阵
```python
A = torch.arange(6).reshape(3, 2)
A
# tensor([[0, 1],
#         [2, 3],
#         [4, 5]])

# 访问矩阵的转置
A.T
# tensor([[0, 2, 4],
#         [1, 3, 5]])
```
Symmetric matrices are the subset of square matrices that are equal to their own transposes
```python
A = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
A == A.T
# tensor([[True, True, True],
#         [True, True, True],
#         [True, True, True]])
```

### 2.3.4 Tensors
向量是一阶张量，矩阵是二阶张量。 张量用特殊字体的大写字母表示 X，Y 和 Z
```python
torch.arange(24).reshape(2, 3, 4)


# 相当于 所有数据分成 2 大块 每个大块 内部 分为 3 个小块 3个小块内部在分 4个

# tensor([[[ 0,  1,  2,  3],
#          [ 4,  5,  6,  7],
#          [ 8,  9, 10, 11]],

#         [[12, 13, 14, 15],
#          [16, 17, 18, 19],
#          [20, 21, 22, 23]]])

```

### 2.3.5 张量算法的基本性质 Basic Properties of Tensor Arithmetic 
两个矩阵的按元素乘法称为Hadamard积（Hadamard product）
```python
A = torch.arange(6, dtype=torch.float32).reshape(2, 3)
B = A.clone()  # Assign a copy of A to B by allocating new memory
A, A + B
# (tensor([[0., 1., 2.],
#          [3., 4., 5.]]),
#  tensor([[ 0.,  2.,  4.],
#          [ 6.,  8., 10.]]))

A * B
# tensor([[ 0.,  1.,  4.],
#         [ 9., 16., 25.]])

# Adding or multiplying a scalar and a tensor produces a result with the same shape as the original tensor. Here, each element of the tensor is added to (or multiplied by) the scalar.
a = 2
X = torch.arange(24).reshape(2, 3, 4)
a + X, (a * X).shape
# (tensor([[[ 2,  3,  4,  5],
#           [ 6,  7,  8,  9],
#           [10, 11, 12, 13]],

#          [[14, 15, 16, 17],
#           [18, 19, 20, 21],
#           [22, 23, 24, 25]]]),
#  torch.Size([2, 3, 4]))
```


### 2.3.6 降维 Reduction 
任意形状张量的元素和
```python
# 调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量
x = torch.arange(3, dtype=torch.float32)
x, x.sum()
# (tensor([0., 1., 2.]), tensor(3.))

# By default, invoking the sum function reduces a tensor along all of its axes, eventually producing a scalar. Our libraries also allow us to specify the axes along which the tensor should be reduced. To sum over all elements along the rows (axis 0), we specify axis=0 in sum. Since the input matrix reduces along axis 0 to generate the output vector, this axis is missing from the shape of the output.

A.shape, A.sum()
# (torch.Size([2, 3]), tensor(15.))
# 输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失
A.shape, A.sum(axis=0).shape, A.sum(axis=0)
# (torch.Size([2, 3]), torch.Size([3]), tensor([3., 5., 7.]))

# 指定axis=1将通过汇总所有列的元素降维（轴1）
A.shape, A.sum(axis=1).shape, A.sum(axis=1)
# (torch.Size([2, 3]), torch.Size([2]), tensor([ 3., 12.]))

A.sum(axis=[0, 1]) == A.sum()  # Same as A.sum()

A.mean(), A.sum() / A.numel()
# (tensor(2.5000), tensor(2.5000))

# 用一个3维的例子加深体会
c=torch.arange(24)
# tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
#         18, 19, 20, 21, 22, 23])
C=c.reshape(2,3,4)
# tensor([[[ 0,  1],
#          [ 2,  3],
#          [ 4,  5]],

#         [[ 6,  7],
#          [ 8,  9],
#          [10, 11]],

#         [[12, 13],
#          [14, 15],
#          [16, 17]],

#         [[18, 19],
#          [20, 21],
#          [22, 23]]])

C.sum(axis=0)
# tensor([[36, 40],
#         [44, 48],
#         [52, 56]])
C.sum(axis=1)
# tensor([[ 6,  9],
#         [24, 27],
#         [42, 45],
#         [60, 63]])
C.sum(axis=2)
# tensor([[ 1,  5,  9],
#         [13, 17, 21],
#         [25, 29, 33],
#         [37, 41, 45]])


C.sum(axis=[0,1])
# tensor([132, 144])
C.sum(axis=[0,2])
# tensor([ 76,  92, 108])
C.sum(axis=[1,2])
# tensor([ 15,  51,  87, 123])

```

### 2.3.7 非降维求和 Non-Reduction Sum
Sometimes it can be useful to keep the number of axes unchanged when invoking the function for calculating the sum or mean. This matters when we want to use the broadcast mechanism.
```python
sum_A = A.sum(axis=1, keepdims=True)
sum_A, sum_A.shape
# (tensor([[ 3.],
#          [12.]]),
#  torch.Size([2, 1]))

A / sum_A
# tensor([[0.0000, 0.3333, 0.6667],
#         [0.2500, 0.3333, 0.4167]])


# 如果我们想沿某个轴计算A元素的累积总和， 比如axis=0（按行计算），可以调用cumsum函数。 此函数不会沿任何轴降低输入张量的维度。
A.cumsum(axis=0)
# tensor([[0., 1., 2.],
#         [3., 5., 7.]])
```

### 2.3.8 Dot Products
Given two vectors their dot product is 
$x · y = \sum_{d}^{i=1}x_iy_i$

```python
y = torch.ones(3, dtype = torch.float32)
x, y, torch.dot(x, y)
# (tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.))

# Equivalently, we can calculate the dot product of two vectors by performing an elementwise multiplication followed by a sum:
torch.sum(x * y)
```

### 2.3.9 矩阵向量积 Matrix-Vector Products
```python
# matrix-vector
A.shape, x.shape, torch.mv(A, x), A@x
# (torch.Size([2, 3]), torch.Size([3]), tensor([ 5., 14.]), tensor([ 5., 14.]))

To express a matrix–vector product in code, we use the mv function. Note that the column dimension of A (its length along axis 1) must be the same as the dimension of x (its length). Python has a convenience operator @ that can execute both matrix–vector and matrix–matrix products (depending on its arguments). Thus we can write A@x.
```

### 2.3.10 矩阵矩阵乘法 matrix-matrix multiplication
```python
B = torch.ones(3, 4)
# mm matrix-matrix
torch.mm(A, B), A@B
# (tensor([[ 3.,  3.,  3.,  3.],
#          [12., 12., 12., 12.]]),
#  tensor([[ 3.,  3.,  3.,  3.],
#          [12., 12., 12., 12.]]))
```

### 2.3.11 范数 Norms
在数学中，范数（Norm）是一种衡量向量空间中向量大小的方式。范数本质上是一个函数，它将向量映射到非负实数集合中。它衡量了向量的大小或长度。
范数可以是多种形式，其中最常见的是 $L^p$范数
* $L^1$范数(曼哈顿距离): 衡量了向量中所有元素绝对值的总和，也被称为曼哈顿距离，因为它类似于在城市中沿着网格街道行走的距离。 $ ||x||_1 = \sum_{i=1}^n|x_i|$
* $L^2$范数(欧几里得距离): 向量元素平方和的平方根，也称为欧几里得距离，它在几何空间中用于测量两点之间的直线距离。$ ||x||_2 = \sqrt[2]{\sum_{n=1}^nx_i^2}$
* $L^p$范数 $ ||x||_p = (\sum_{i=1}^n|x_i|^p)^{\frac{1}{p}} $ 其中 p>=1

A norm is a function ||·|| that maps a vector to a scalar and satisfies the following three properties:
1. $ || α x|| = |α| ||x|| $
2. $ |||x+y|| <= ||x|| + ||y|| $
3. $ ||x|| > 0 $ for all $ x != 0 $


不同的 Lp 范数在数学和工程领域有不同的应用，选择不同的范数可以根据具体问题的特点来衡量向量的大小或距离

```python

u = torch.tensor([3.0, -4.0])

# l1 norm
torch.abs(u).sum()

# l2 norm
torch.norm(u)

```

类似于向量的 $L_2$范数, 矩阵 $ X \in R $ 的Frobenius范数 是 矩阵元素平方和的平方根
$ ||X||_F = \sqrt{\sum_{i=1}^m\sum_{j=1}^nx_{ij}^2} $ 调用以下函数将计算矩阵的Frobenius范数。
```python
torch.norm(torch.ones((4, 9)))
# tensor(6.)
```

### 2.3.12 范数和目标
在深度学习中，我们经常试图解决优化问题： 最大化分配给观测数据的概率; 最小化预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。

目标或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。
## 2.1 数据操作
n维数组也称 张量(tensor)。 无论使用哪个深度学习框架，它的张量类（在MXNet中为ndarray， 在PyTorch和TensorFlow中为Tensor）都与Numpy的ndarray类似。 但深度学习框架又比Numpy的ndarray多一些重要功能： 首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算； 其次，张量类支持自动微分。

### 2.1.1 入门
张量表示一个由数值组成的数组，这个数组可能有多个维度。 具有一个轴的张量对应数学上的向量（vector）； 具有两个轴的张量对应数学上的矩阵（matrix）； 具有两个轴以上的张量没有特殊的数学名称

```python
import torch
# range 创建一个行向量
x = torch.arange(12,dtype=torch.float32)
x
# tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])

# 访问张量的shape来获得形状
x.numel()
# 12

# 张量中元素总数 可以检查 张量的 size
x.shape
# torch.Size([12])

# 改变一个张量的形状 reshape 函数
X = x.reshape(3, 4)
X = x.reshape(3, -1)
X = x.reshape(-1, 4)
tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.]])

# 全0 
torch.zeros((2, 3, 4))
# tensor([[[0., 0., 0., 0.],
#          [0., 0., 0., 0.],
#          [0., 0., 0., 0.]],

#         [[0., 0., 0., 0.],
#          [0., 0., 0., 0.],
#          [0., 0., 0., 0.]]])

# 全1
torch.ones((2, 3, 4))
# tensor([[[1., 1., 1., 1.],
#          [1., 1., 1., 1.],
#          [1., 1., 1., 1.]],

#         [[1., 1., 1., 1.],
#          [1., 1., 1., 1.],
#          [1., 1., 1., 1.]]])

# 有时我们想通过从某个特定的概率分布中随机采样来得到张量中每个元素的值。 例如，当我们构造数组来作为神经网络中的参数时，我们通常会随机初始化参数的值。 以下代码创建一个形状为（3,4）的张量。 其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样
torch.randn(3, 4)
# tensor([[ 0.1351, -0.9099, -0.2028,  2.1937],
#         [-0.3200, -0.7545,  0.8086, -1.8730],
#         [ 0.3929,  0.4931,  0.9114, -0.7072]])

# 从 py 数组 到 tensor
torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
# tensor([[2, 1, 4, 3],
#         [1, 2, 3, 4],
#         [4, 3, 2, 1]])
```

### 2.1.2 运算符
数据上执行数学运算，其中最简单且最有用的操作是按元素（elementwise）运算。
```python
x = torch.tensor([1.0, 2, 4, 8])
y = torch.tensor([2, 2, 2, 2])
x + y, x - y, x * y, x / y, x ** y # **运算符是求幂运算

# (tensor([ 3.,  4.,  6., 10.]),
#  tensor([-1.,  0.,  2.,  6.]),
#  tensor([ 2.,  4.,  8., 16.]),
#  tensor([0.5000, 1.0000, 2.0000, 4.0000]),
#  tensor([ 1.,  4., 16., 64.]))

# 自然常数 e e的1次方 e的2次方 e的4次方 e的8次方
torch.exp(x)
# tensor([1.0000e+00, 2.7183e+00, 7.3891e+00, 2.0086e+01, 5.4598e+01, 1.4841e+02,
#         4.0343e+02, 1.0966e+03, 2.9810e+03, 8.1031e+03, 2.2026e+04, 5.9874e+04])

# 张量连结（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量
# 需要提供张量列表并给出沿哪个轴连接。下面的例子分别演示了当我们沿行（轴-0，形状的第一个元素） 和按列（轴-1，形状的第二个元素）连结两个矩阵时，会发生什么情况
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)
# (tensor([[ 0.,  1.,  2.,  3.],
#          [ 4.,  5.,  6.,  7.],
#          [ 8.,  9., 10., 11.],
#          [ 2.,  1.,  4.,  3.],
#          [ 1.,  2.,  3.,  4.],
#          [ 4.,  3.,  2.,  1.]]),
#  tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
#          [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
#          [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))

# 通过逻辑运算符构建二元张量。 以X == Y为例： 对于每个位置，如果X和Y在该位置相等，则新张量中相应项的值为1。 这意味着逻辑语句X == Y在该位置处为真，否则该位置为0。
X==Y
# tensor([[False,  True, False,  True],
#         [False, False, False, False],
#         [False, False, False, False]])

# 对张量中的所有元素进行求和，会产生一个单元素张量
X.sum()
# tensor(66.)
```

### 2.1.3 广播机制
上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，即使形状不同，我们仍然可以通过调用 广播机制（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下:

1. expand one or both arrays by copying elements along axes with length 1 so that after this transformation, the two tensors have the same shape 沿着长度为 1 的轴复制元素来扩展一个或两个张量，以便在这种转换之后，两个张量具有相同的形状
2. perform an elementwise operation on the resulting array

大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子
```python
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
a, b
# (tensor([[0],
#          [1],
#          [2]]),
#  tensor([[0, 1]]))

# a 和 b 分别是 3 x 1 和 1 x 2 矩阵，如果让它们相加，它们的形状不匹配。 我们将两个矩阵广播为一个更大的矩阵，如下所示：矩阵a将复制列， 矩阵b将复制行，然后再按元素相加。
a+b
# tensor([[0, 1],
#         [1, 2],
#         [2, 3]])

```

### 2.1.4 索引和切片
张量中的元素可以通过索引访问。访问得到的也都还是 tensor。
```python
# [-1] selects the last row and [1:3] selects the second and third rows.
X[-1],X[1:3]
# (tensor([ 8.,  9., 10., 11.]),
#  tensor([[ 4.,  5.,  6.,  7.],
#          [ 8.,  9., 10., 11.]]))

X[1, 2] = 17
X
# tensor([[ 0.,  1.,  2.,  3.],
#         [ 4.,  5., 17.,  7.],
#         [ 8.,  9., 10., 11.]])
X[:2, :] = 12
X
# tensor([[12., 12., 12., 12.],
#         [12., 12., 12., 12.],
#         [ 8.,  9., 10., 11.]])
```
Tensors in TensorFlow are immutable, and cannot be assigned to. Variables in TensorFlow are mutable containers of state that support assignments. Keep in mind that gradients in TensorFlow do not flow backwards through Variable assignments. 请记住，TensorFlow中的梯度不会通过Variable反向传播。除了为整个Variable分配一个值之外，我们还可以通过索引来写入Variable的元素。
```python
X_var = tf.Variable(X)
X_var[1, 2].assign(9)
X_var
# <tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=
# array([[ 0.,  1.,  2.,  3.],
#        [ 4.,  5.,  9.,  7.],
#        [ 8.,  9., 10., 11.]], dtype=float32)>

# X_var[1, 2]返回的是一个EagerTensor对象，这是因为它是从一个TensorFlow的Variable对象中获取的特定位置的值。虽然它返回的也是一个EagerTensor对象，但是Variable对象本身支持对其元素进行赋值操作。

# 这种情况下的EagerTensor对象是可以与Variable对象的特性相互作用的，因为它是Variable对象的一部分，允许对其进行赋值操作。而tf.range(12)返回的EagerTensor对象是一个普通的张量，是不可变的，因此不能直接对其元素进行赋值操作。

```

为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值
```python
X_var = tf.Variable(X)
X_var[0:2, :].assign(tf.ones(X_var[0:2,:].shape, dtype = tf.float32) * 12)
X_var
# <tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=
# array([[12., 12., 12., 12.],
#        [12., 12., 12., 12.],
#        [ 8.,  9., 10., 11.]], dtype=float32)>
```

### 2.1.5 节省内存
1. 我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；
2. 如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。

```python
Z = torch.zeros_like(Y)
print('id(Z):', id(Z))
Z[:] = X + Y
print('id(Z):', id(Z))
# id(Z): 140381179266448
# id(Z): 140381179266448
```






```python
Z = tf.Variable(tf.zeros_like(Y))
print('id(Z):', id(Z))
Z.assign(X + Y)
print('id(Z):', id(Z))
# id(Z): 140005044197312
# id(Z): 140005044197312
```
即使你将状态持久存储在Variable中， 你也可能希望避免为不是模型参数的张量过度分配内存，从而进一步减少内存使用量。由于TensorFlow的Tensors是不可变的，而且梯度不会通过Variable流动， 因此TensorFlow没有提供一种明确的方式来原地运行单个操作。

TensorFlow提供了tf.function修饰符， 将计算封装在TensorFlow图中，该图在运行前经过编译和优化。 这允许TensorFlow删除未使用的值，并复用先前分配的且不再需要的值。 这样可以最大限度地减少TensorFlow计算的内存开销
```python
@tf.function
def computation(X, Y):
    Z = tf.zeros_like(Y)  # 这个未使用的值将被删除
    A = X + Y  # 当不再需要时，分配将被复用
    B = A + Y
    C = B + Y
    return C + Y

computation(X, Y)

# <tensor: shape=(3, 4), dtype=float32, numpy=
# array([[ 8.,  9., 26., 27.],
#        [24., 33., 42., 51.],
#        [56., 57., 58., 59.]], dtype=float32)>
```

### 2.1.6 转换为其他 Python 对象
转换后的结果不共享内存。 这个小的不便实际上是非常重要的：当在CPU或GPU上执行操作的时候， 如果Python的NumPy包也希望使用相同的内存块执行其他操作，人们不希望停下计算来等它。
```python
A = X.numpy()
B = torch.from_numpy(A)
type(A), type(B)

# (numpy.ndarray, torch.Tensor)

# To convert a size-1 tensor to a Python scalar, we can invoke the item function or Python's built-in functions.
a = torch.tensor([3.5])
a, a.item(), float(a), int(a)
# (tensor([3.5000]), 3.5, 3.5, 3)
```
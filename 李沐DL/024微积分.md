## 2.4 Calculus
The limiting procedure (method of exhaustion) is at the root of both differential calculus(微分) and integral calculus(积分). 

在微分学最重要的应用是优化问题，即考虑如何把事情做到最好。This comes in handy for the optimization problems that we face in deep learning, where we repeatedly update our parameters in order to decrease the loss function. 在深度学习中，我们“训练”模型，不断更新它们，使它们在看到越来越多的数据时变得越来越好。 通常情况下，变得更好意味着最小化一个损失函数（loss function）。最终，我们真正关心的是生成一个模型，它能够在从未见过的数据上表现良好。 但“训练”模型只能将模型与我们实际能看到的数据相拟合。 因此，我们可以将拟合模型的任务分解为两个关键问题：
* 优化（optimization）：用模型拟合观测数据的过程。
* 泛化（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。

### 2.4.1 导数和微分 Derivatives and Differentiation
Derivatives can tell us how rapidly a loss function would increase or decrease were we to increase or decrease each parameter by an infinitesimally small amount. 简而言之，对于每个参数， 如果我们把这个参数增加或减少一个无穷小的量，可以知道损失会以多快的速度增加或减少。

### 2.4.2 偏导数和梯度 Partial Derivatives and Gradients

在一元函数中，梯度就是导数。而在多元函数中，梯度是一个向量，每个分量代表对应自变量方向上的偏导数。 f(x,y,z), 其梯度记作 \
$\displaystyle ∇f=(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial x}, \frac{\partial f}{\partial x})$ \
这个梯度向量在某一点上的取值告诉我们函数在该点上增长最快的方向和速率。梯度的方向是函数值增长最快的方向，梯度的大小（模）表示增长速率，即函数值在该方向上的变化率。
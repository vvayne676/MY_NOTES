## 3.3 Synthetic Regression Data
### 3.3.1 Generating the Dataset 
```python
%matplotlib inline
import random
import torch
from d2l import torch as d2l

class SyntheticRegressionData(d2l.DataModule):  #@save
    """Synthetic data for linear regression."""
    def __init__(self, w, b, noise=0.01, num_train=1000, num_val=1000,
                 batch_size=32):
        super().__init__()
        self.save_hyperparameters()
        n = num_train + num_val
        self.X = torch.randn(n, len(w))
        noise = torch.randn(n, 1) * noise
        self.y = torch.matmul(self.X, w.reshape((-1, 1))) + b + noise
# w = weights, it is a tensor
# b = bias, it is a scalar
# noise 参数代表噪声水平，它是一个标量，用于生成随机噪声，这些噪声会被添加到线性关系生成的目标值 y 上，以模拟现实世界数据中的随机波动或测量误差。
# num_train 和 num_val 分别代表训练集和验证集中的样本数量。
# batch_size 是每个批次中的样本数量，用于在训练模型时分批处理数据

# y = Xw + b + noise
# X 是特征矩阵
# y 是目标向量
data = SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)

# Each row in features consists of a vector in  and each row in labels is a scalar
print('features:', data.X[0],'\nlabel:', data.y[0])
# features: tensor([0.9026, 1.0264])
# label: tensor([2.5148])
```

### 3.3.2 Reading the Dataset
训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型。 由于这个过程是训练机器学习算法的基础，所以有必要定义一个函数， 该函数能打乱数据集中的样本并以小批量方式获取数据。

在下面的代码中，我们定义一个data_iter函数， 该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为batch_size的小批量。 每个小批量包含一组特征和标签
```python
@d2l.add_to_class(SyntheticRegressionData)
def get_dataloader(self, train):
    if train:
        indices = list(range(0, self.num_train))
        # The examples are read in random order
        random.shuffle(indices)
    else:
        indices = list(range(self.num_train, self.num_train+self.num_val))
    for i in range(0, len(indices), self.batch_size):
        batch_indices = torch.tensor(indices[i: i+self.batch_size])
        yield self.X[batch_indices], self.y[batch_indices]

X, y = next(iter(data.train_dataloader()))
# (tensor([[-1.4009, -0.2685],
#         [-2.3944, -0.3889],
#         [-0.1355,  0.7544],
#         [-0.5883,  1.0033],
#         [-1.4487, -0.9779],
#         [-0.7249, -0.3742],
#         [-1.4835, -0.8088],
#         [ 0.9829,  1.1275],
#         [ 1.0711, -0.7312],
#         [ 0.2454, -0.7665],
#         [ 0.2892,  0.3243],
#         [ 0.9030,  0.5761],
#         [-0.4372, -1.1466],
#         [ 2.4412,  2.3353],
#         [ 0.9376,  0.5214],
#         [-0.4844,  2.0064],
#         [ 0.6125,  1.3573],
#         [ 0.0743,  2.6550],
#         [ 0.4190,  0.1096],
#         [ 2.3492,  0.2783],
#         [ 0.0464, -1.0059],
#         [-0.3922, -0.5252],
#         [-1.8960, -1.1383],
#         [ 0.1146,  1.3560],
#         [-1.7498,  1.0631],
#         [ 0.8890, -0.4041],
#         [ 0.3227, -1.6608],
#         [ 0.5238, -1.0800],
#         [ 0.2681, -1.1318],
#         [ 1.2765, -0.0818],
#         [ 0.1778,  0.2437],
#         [-0.5254,  0.8926]]), tensor([[ 2.3016],
#         [ 0.7276],
#         [ 1.3778],
#         [-0.4003],
#         [ 4.6230],
#         [ 4.0228],
#         [ 3.9869],
#         [ 2.3172],
#         [ 8.8339],
#         [ 7.3156],
#         [ 3.6977],
#         [ 4.0582],
#         [ 7.2274],
#         [ 1.1380],
#         [ 4.3114],
#         [-3.5925],
#         [ 0.7929],
#         [-4.6672],
#         [ 4.6715],
#         [ 7.9474],
#         [ 7.7273],
#         [ 5.1978],
#         [ 4.2797],
#         [-0.1736],
#         [-2.9235],
#         [ 7.3583],
#         [10.4985],
#         [ 8.9308],
#         [ 8.5830],
#         [ 7.0219],
#         [ 3.7213],
#         [ 0.1158]]))

```

### 3.3.3 Concise Implementation of the Data Loader
Rather than writing our own iterator, we can call the existing API in a framework to load data. As before, we need a dataset with features X and labels y. Beyond that, we set batch_size in the built-in data loader and let it take care of shuffling examples efficiently. loader is sort a container of data or tensor
```python
@d2l.add_to_class(d2l.DataModule)  #@save
def get_tensorloader(self, tensors, train, indices=slice(0, None)):
    tensors = tuple(a[indices] for a in tensors)
    dataset = torch.utils.data.TensorDataset(*tensors)
    return torch.utils.data.DataLoader(dataset, self.batch_size,
                                       shuffle=train)

@d2l.add_to_class(SyntheticRegressionData)  #@save
def get_dataloader(self, train):
    i = slice(0, self.num_train) if train else slice(self.num_train, None)
    return self.get_tensorloader((self.X, self.y), train, i)

X, y = next(iter(data.train_dataloader()))
print('X shape:', X.shape, '\ny shape:', y.shape)
# X shape: torch.Size([32, 2])
# y shape: torch.Size([32, 1])
```
### 3.3.4 Summary
Data loaders are a convenient way of abstracting out the process of loading and manipulating data.

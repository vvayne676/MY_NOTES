## 2.5 Automatic Differentiation
As we pass data through each successive function, the framework builds a computational graph that tracks how each value depends on others. To calculate derivatives, automatic differentiation works backwards through this graph applying the chain rule. The computational algorithm for applying the chain rule in this fashion is called backpropagation.

### 2.5.1 A Simple Function
梯度是一个向量，用于表示多变量函数在给定点上的变化率和方向。对于多元函数 f(x1, x2, ..., xn)，其梯度是一个向量，表示函数在每个自变量方向上的偏导数。

Before we calculate the gradient of `y` with respect to `x`, we need a place to store it. In general, we avoid allocating new memory every time we take a derivative because deep learning requires successively computing derivatives with respect to the same parameters a great many times, and we might risk running out of memory. Note that the gradient of a scalar-valued function with respect to a vector `x` is vector-valued with the same shape as `x`.
注意，一个标量函数关于向量`x`的梯度是向量，并且与`x`具有相同的形状。
```python
import torch

x = torch.arange(4.0)
x # tensor([0., 1., 2., 3.])

x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)
x.grad  # 默认值是None

y = 2 * torch.dot(x, x)
y # tensor(28., grad_fn=<MulBackward0>)

# We can now take the gradient of y with respect to x by calling its backward method. Next, we can access the gradient via x's grad attribute.
y.backward()
x.grad

# We already know that the gradient of the function is 4x
x.grad == 4 * x


# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值
x.grad.zero_()

y = x.sum()
y.backward()
x.grad # tensor([1., 1., 1., 1.])
```
标量的求导规则也可以扩展应用于向量甚至张量。在多维空间中，这些规则被用来计算向量或张量的导数，这在数学上被称为梯度（对于向量）和雅可比矩阵（对于向量函数）或海森矩阵（对于标量函数的二阶导数）。

对于向量函数，我们通常计算它们相对于另一个向量的导数，这可以通过分别对每个分量求导来完成。这种操作产生了一个导数矩阵，其中包含了所有可能的偏导数。

对于张量（多维数组），求导的概念也是相似的，但涉及到更高维度的数据结构。在机器学习和深度学习中，这些概念是反向传播算法的基础，该算法用于计算损失函数相对于网络参数的梯度。

反向传播算法是一种在神经网络中用于训练模型的核心算法。它的目的是计算损失函数（一个衡量模型预测与实际结果差异的标量值）相对于网络中每个参数（权重和偏置）的梯度。这些梯度提供了关于如何调整参数以减少损失并改进模型性能的信息。

反向传播算法的基本步骤如下：

前向传播：输入数据通过网络，每一层的输出作为下一层的输入，直到最终产生输出。

计算损失：使用损失函数计算网络输出与真实标签之间的差异。

反向传播：计算损失函数相对于网络参数的梯度。这是通过链式法则进行的，它是微积分中的一个基本规则，允许我们计算复合函数的导数。

参数更新：使用计算出的梯度来更新网络参数，通常是通过梯度下降或其变体进行。

在反向传播的过程中，梯度是通过以下步骤计算的：

首先计算输出层的梯度，即损失函数相对于输出层激活的导数。
然后，使用链式法则反向工作，逐层计算隐藏层的梯度。对于每一层，我们计算该层激活函数相对于其输入的导数，并将其与来自上一层的梯度相乘。
这个过程一直持续到输入层。
在实践中，这意味着我们从输出层开始，逐步向后计算每个参数的梯度，直到达到输入层。这些梯度随后被用于更新网络中的权重和偏置，通常是通过梯度下降算法或其变体，如随机梯度下降（SGD）、Adam等。

反向传播算法的效率和有效性使得它成为训练深度神经网络的标准方法

### 2.5.2 非标量的反向传播 Backward for Non-Scalar Variables
When y is a vector, the most natural representation of the derivative of y with respect to a vector x is a matrix called the Jacobian that contains the partial derivatives of each component of y with respect to each component of x. 对于高阶和高维的y和x，求导的结果可以是一个高阶张量。

然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中）， 但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。

```python
# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。
# 本例只想求偏导数的和，所以传递一个1的梯度是合适的
x.grad.zero_()
y = x * x
y.backward(torch.ones(len(x))) # Faster: y.sum().backward()
x.grad
```

### 2.5.3 分离计算
有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设y是作为x的函数计算的(y=x*x)，而z(z=y*x)则是作为y和x的函数计算的。 想象一下，我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数， 并且只考虑到x在y被计算后发挥的作用。\
这里可以分离y来返回一个新变量u，该变量与y具有相同的值， 但丢弃计算图中如何计算y的任何信息。 换句话说，梯度不会向后流经u到x。 因此，下面的反向传播函数计算z=u*x关于x的偏导数，同时将u作为常数处理， 而不是z=x*x*x关于x的偏导数

```python
x.grad.zero_()
y = x * x
u = y.detach()
z = u * x

z.sum().backward()
x.grad == u
# tensor([True, True, True, True])

# 由于记录了y的计算结果，我们可以随后在y上调用反向传播， 得到y=x*x关于的x的导数，即2*x。
x.grad.zero_()
y.sum().backward()
x.grad == 2 * x
```

### 2.5.4 Python控制流的梯度计算
```python
def f(a):
    b = a * 2
    while b.norm() < 1000:
        b = b * 2
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b
    return c

a = torch.randn(size=(), requires_grad=True)
d = f(a)
d.backward()
# 我们现在可以分析上面定义的f函数。 请注意，它在其输入a中是分段线性的。 换言之，对于任何a，存在某个常量标量k，使得f(a)=k*a，其中k的值取决于输入a，因此可以用d/a验证梯度是否正确。
a.grad == d / a
# tensor(True)
```

### 2.5.5 小结
深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。

### 2.5.6 为什么梯度计算调用 backward 一定要 把 张量 sum一下
梯度的概念通常是针对标量函数相对于其一个或多个变量的导数。

换句话说，梯度是一个向量，它指向多变量函数增长最快的方向，并且其大小是增长率。

当我们在训练神经网络时，我们通常关注的是损失函数（一个标量函数）相对于模型参数（可以是向量、矩阵或更高维张量）的梯度。

如果你尝试对一个非标量张量直接计算梯度，你实际上是在尝试计算一个张量相对于另一个张量的导数，这在数学上是没有定义的。这就是为什么在实践中，我们通常会将非标量张量通过某种形式的聚合（如求和或取平均）转换为标量，然后再计算梯度。
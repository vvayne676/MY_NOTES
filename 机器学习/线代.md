# 为什么各种矩阵操作后能够得到检索结果 
各种矩阵操作能够得到检索结果的原因在于这些操作能够捕捉和表示文本数据中的语义信息，并将其转换为计算机可以处理的数学形式。以下是这一过程的详细解释：

1. **向量空间模型**：
   - 在NLP中，文本通常被表示为向量，这些向量存在于一个被称为向量空间的抽象数学空间中。
   - 每个维度代表文本的某个特征，例如特定的单词或概念。
   - 通过这种方式，文本数据被转换为数值数据，使得可以应用数学和统计方法进行处理。

2. **语义相似性**：
   - 在向量空间中，语义上相似的词或文档的向量会彼此接近。
   - 这种接近性可以通过各种相似度度量来量化，如余弦相似度、欧几里得距离等。
   - 因此，通过计算向量之间的相似度，我们可以估计它们代表的文本之间的语义相似性。

3. **矩阵运算**：
   - 矩阵是向量的一种扩展，可以表示一组向量或整个数据集。
   - 在NLP中，矩阵运算（如矩阵乘法、奇异值分解等）被用来处理和转换这些向量，以提取特征、降维、构建语义空间等。
   - 这些运算使得模型能够高效地处理大量数据，并在这些数据上执行复杂的操作。

4. **检索机制**：
   - 在检索任务中，查询被转换为一个向量，而待检索的文档或信息也被表示为向量。
   - 通过矩阵运算，可以快速计算查询向量与所有候选文档向量之间的相似度。
   - 然后，根据相似度分数对候选文档进行排序，最相似的文档会被选为检索结果。

5. **高效计算**：
   - 矩阵运算是高度优化的，可以利用现代计算硬件（如GPU和TPU）进行快速并行计算。
   - 这使得即使在大规模数据集上，检索任务也能在短时间内完成。

总之，矩阵运算在NLP中的作用是将文本数据转换为数值形式，并通过数学方法来处理这些数据。这些操作使得计算机能够理解和比较文本的语义内容，从而实现有效的信息检索。
## Normalization（标准化/归一化）

Normalization (also known as min-max scaling) rescales the values of a feature to a specific range, typically between 0 and 1. The formula for normalization is as follows:\
x_normalized = (x - min(x)) / (max(x) - min(x))

Standardization (also known as z-score normalization) transforms the values of a feature to have a mean of 0 and a standard deviation of 1. The formula for standardization is as follows:\
x_standardized = (x - mean(x)) / standard_deviation(x)

In practice, the terms normalization and standardization are sometimes used interchangeably, especially in the context of machine learning and data analysis. It's important to clarify the specific rescaling technique being used to avoid any confusion.

## Regularization（正则化）

1. 减少过拟合：正则化通过限制模型的复杂度，减少了模型对训练数据中噪声的拟合，从而降低过拟合的风险。

2. 提高泛化能力��通过防止过拟合，正则化帮助模型在新的、未见过的数据上有更好的表现，即提高了模型的泛化能力。

3. 选择特征：某些类型的正则化（如L1正则化）可以促进模型权重的稀疏性，这意味着它可以将某些不重要的特征的权重降至零，从而起到特征选择的作用。

4. 处理共线性：在存在高度相关特征的数据中，正则化可以帮助减少这些特征之间的冗余，防止模型参数估计出现不稳定的情况。

5. 改善条件数：在优化问题中，正则化可以改善问题的条件数，使得优化算法更加稳定和高效。

6. 引入先验知识：正则化项可以被看作是对模型参数的先验分布，通过选择合适的正则化项，可以将领域知识或者对问题的先验理解融入模型中。

常见的正则化方法包括：

* L1正则化（Lasso）：在损失函数中添加权重的绝对值之和作为惩罚项，倾向于产生稀疏权重矩阵。
* L2正则化（Ridge）：在损失函数中添加权重的平方和作为惩罚项，通常会使得权重值较小但不为零。
* 弹性网（Elastic Net）：结合了L1和L2正则化，同时具有选择特征和稳定性的优点。
在实际应用中，正则化的强度通常由一个超参数控制，这个超参数需要通过交叉验证等方法来选择最优值。

## Sigmod/Logistic 函数 (二分类)

将$(-\infty,+\infty)$范围内的数值映射成为一个(0,1)区间的数值，一个(0,1)区间的数值恰好可以用来表示概率 

$g(z)=\frac{1}{1+e^{-z}}$

比如，在互联网广告和推荐系统中，曾广泛使用Sigmod函数来预测某项内容是否有可能被点击。Sigmoid函数输出值越大，说明这项内容被用户点击的可能性越大，越应该将该内容放置到更加醒目的位置。

## Softmax 函数 (多分类)

对于多分类问题，一种常用的方法是Softmax函数，它可以预测每个类别的概率

$ Softmax(z_i)=\frac{exp(z_i)}{\sum_j exp(z_j)}$

if we add a constant to all the input values, the result will be the same. 